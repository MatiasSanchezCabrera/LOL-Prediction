{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import subprocess\n",
    "import pylab as pl\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the Accuracy for the Naive Approach (the team with more gold wins)\n",
    "def naive_accuracy(X,y):    \n",
    "    blue = len(X[X['Gold']>0][y==100])\n",
    "    red  = len(X[X['Gold']<0][y==200])\n",
    "    games = len(X)\n",
    "    print 'Naive Accuracy: {:.2f}%'.format(float(blue+red)/games*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nTraining time: {:.3f} secs\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_labels(clf, features, target,text):\n",
    "    if text:\n",
    "        print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    if text:\n",
    "        print \"Done!\\nPrediction time: {:.3f} secs\".format(end - start)\n",
    "    if text:\n",
    "        print \"Confusion Matrix:\\n {}\".format(confusion_matrix(target, y_pred))\n",
    "    return accuracy_score(target.values, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_curve(clf, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "\n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.round(np.linspace(100, len(X_train), 50))\n",
    "    train_f1 = np.zeros(len(sizes))\n",
    "    test_f1 = np.zeros(len(sizes))\n",
    "\n",
    "    for i, s in enumerate(sizes):\n",
    "        # Create and fit the decision tree regressor model\n",
    "        clf.fit(X_train[:int(s)], y_train[:int(s)])\n",
    "\n",
    "        # Find the performance on the training and testing set\n",
    "        train_f1[i] = predict_labels(clf,X_train,y_train,False)\n",
    "        test_f1[i] = predict_labels(clf,X_test,y_test,False)\n",
    "\n",
    "\n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(sizes, train_f1, test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_curve_graph(sizes, train_f1, test_f1):\n",
    "    \"\"\"Plot training and test error as a function of the training size.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Performance vs Training Size')\n",
    "    pl.plot(sizes, test_f1, lw=2, label = 'Test F1 Score')\n",
    "    pl.plot(sizes, train_f1, lw=2, label = 'Train F1 Score')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Training Size')\n",
    "    pl.ylabel('F1 Score')\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_curve(clf, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "    \n",
    "    parameter = [50,100,150,200,250,300]\n",
    "    train_f1 = np.zeros(len(parameter))\n",
    "    test_f1 = np.zeros(len(parameter))\n",
    "    \n",
    "    for i, d in enumerate(parameter):\n",
    "        # Create and fit the decision tree regressor model\n",
    "        clf.min_samples_split = d\n",
    "        clf.fit(X_train, y_train)\n",
    "        # Find the performance on the training and testing set\n",
    "        train_f1[i] = predict_labels(clf,X_train,y_train,False)\n",
    "        test_f1[i] = predict_labels(clf,X_test,y_test,False)\n",
    "\n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(parameter, train_f1, test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_curve_graph(n_estimators, train_f1, test_f1):\n",
    "    \"\"\"Plot training and test error as a function of the depth of the decision tree learn.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Max Depth')\n",
    "    pl.plot(max_depth, test_f1, lw=2, label = 'test error')\n",
    "    pl.plot(max_depth, train_f1, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Max Depth')\n",
    "    pl.ylabel('Error')\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_importances(estimator):\n",
    "    \"\"\"Retrieve or aggregate feature importances from estimator\"\"\"\n",
    "    if hasattr(estimator, \"feature_importances_\"):\n",
    "        importances = estimator.feature_importances_\n",
    "\n",
    "    elif hasattr(estimator, \"coef_\"):\n",
    "        if estimator.coef_.ndim == 1:\n",
    "            importances = np.abs(estimator.coef_)\n",
    "\n",
    "        else:\n",
    "            importances = np.sum(np.abs(estimator.coef_), axis=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The underlying estimator %s has no `coef_` or \"\n",
    "            \"`feature_importances_` attribute. Either pass a fitted estimator\"\n",
    "            \" to SelectFromModel or call fit before calling transform.\"\n",
    "            % estimator.__class__.__name__)\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'matchId', u'matchVersion', u'region', u'queueType', u'matchCreation',\n",
      "       u'matchDuration', u'winner', u'Gold', u'Dragons', u'RiftHerald',\n",
      "       u'Wards', u'DestroyedWards', u'TopTurrets', u'MidTurrets',\n",
      "       u'BotTurrets', u'Inhibitor', u'Kills', u'Assists', u'Cs', u'Jg', u'Xp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Ready csv with game data\n",
    "game_data = pd.read_csv('gameData_Diffs_10Minutes.csv',keep_default_na=False)\n",
    "print game_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 64905 samples\n",
      "Test set: 21635 samples\n"
     ]
    }
   ],
   "source": [
    "#game_data = game_data[abs(game_data['Gold'])<=1000]\n",
    "\n",
    "# Data Set for learning\n",
    "X_all = game_data[game_data.columns[-14:]]\n",
    "y_all = game_data['winner']\n",
    "\n",
    "X_all = pd.DataFrame(preprocessing.scale(X_all), index = X_all.index, columns = X_all.columns)\n",
    "\n",
    "# Separate data into train/test samples\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all,y_all,test_size=0.25)\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: \n",
      "Naive Accuracy: 70.16%\n",
      "Train set: \n",
      "Naive Accuracy: 70.07%\n",
      "Test set: \n",
      "Naive Accuracy: 70.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "print 'Overall: '\n",
    "naive_accuracy(X_all,y_all)\n",
    "print 'Train set: '\n",
    "naive_accuracy(X_train,y_train)\n",
    "print 'Test set: '\n",
    "naive_accuracy(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AdaBoostClassifier...\n",
      "Done!\n",
      "Training time: 2.018 secs\n"
     ]
    }
   ],
   "source": [
    "# Train Classifier\n",
    "#clf = LogisticRegression(solver='sag')\n",
    "#clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf = AdaBoostClassifier()\n",
    "#clf = GradientBoostingClassifier()\n",
    "#clf = SVC()\n",
    "train_classifier(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using GridSearchCV...\n",
      "Done!\n",
      "Prediction time: 0.713 secs\n",
      "Confusion Matrix:\n",
      " [[22278  9823]\n",
      " [ 9321 23483]]\n",
      "Accuracy for training set: 70.5045836222\n",
      "Predicting labels using GridSearchCV...\n",
      "Done!\n",
      "Prediction time: 0.208 secs\n",
      "Confusion Matrix:\n",
      " [[7587 3307]\n",
      " [3071 7670]]\n",
      "Accuracy for test set: 70.5199907557%\n"
     ]
    }
   ],
   "source": [
    "# Grid Search implementation to check the best configuration for Ada Boost \n",
    "tuned_parameters = [{'n_estimators': [1,10,50,100], 'learning_rate': [0.1, 0.3, 0.5, 0.7, 1]}]\n",
    "clf = GridSearchCV(AdaBoostClassifier(),param_grid=tuned_parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate Classifier's performance on Train Data\n",
    "print \"Accuracy for training set: {}\".format(predict_labels(clf, X_train, y_train, True))\n",
    "\n",
    "# Evaluate Classifier's performance on Test Data\n",
    "print \"Accuracy for test set: {}%\".format(predict_labels(clf, X_test, y_test, True))\n",
    "\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.1, n_estimators=100, random_state=None)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using GridSearchCV...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7cc804955053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate Classifier's performance on Train Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Accuracy for training set: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate Classifier's performance on Test Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Accuracy for test set: {}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0e1007da8db0>\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(clf, features, target, text)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Predicting labels using {}...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/metaestimators.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \"\"\"\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "# Evaluate Classifier's performance on Train Data\n",
    "print \"Accuracy for training set: {}\".format(predict_labels(clf, X_train, y_train, True))\n",
    "\n",
    "# Evaluate Classifier's performance on Test Data\n",
    "print \"Accuracy for test set: {}%\".format(predict_labels(clf, X_test, y_test, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance\n",
      "Gold = 52.00\n",
      "Dragons = 8.00\n",
      "RiftHerald = 2.00\n",
      "Wards = 0.00\n",
      "DestroyedWards = 0.00\n",
      "TopTurrets = 0.00\n",
      "MidTurrets = 0.00\n",
      "BotTurrets = 2.00\n",
      "Inhibitor = 0.00\n",
      "Kills = 0.00\n",
      "Assists = 0.00\n",
      "Cs = 10.00\n",
      "Jg = 6.00\n",
      "Xp = 20.00\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "importance = get_feature_importances(clf)\n",
    "print 'Feature importance'\n",
    "for col in X_all.columns:\n",
    "    print col + ' = ' + str(\"{0:.2f}\".format(importance[x]*100))\n",
    "    x = x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultados = pd.DataFrame()\n",
    "resultados['winner'] = clf.predict(X_all)\n",
    "resultados.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
