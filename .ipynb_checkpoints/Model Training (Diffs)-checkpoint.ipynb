{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import subprocess\n",
    "import pylab as pl\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the Accuracy for the Naive Approach (the team with more gold wins)\n",
    "def naive_accuracy(X,y):    \n",
    "    blue = len(X[X['Gold']>0][y==100])\n",
    "    red  = len(X[X['Gold']<0][y==200])\n",
    "    games = len(X)\n",
    "    print 'Naive Accuracy: {:.2f}%'.format(float(blue+red)/games*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nTraining time: {:.3f} secs\".format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_labels(clf, features, target,text):\n",
    "    if text:\n",
    "        print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    if text:\n",
    "        print \"Done!\\nPrediction time: {:.3f} secs\".format(end - start)\n",
    "    if text:\n",
    "        print \"Confusion Matrix:\\n {}\".format(confusion_matrix(target, y_pred))\n",
    "    return accuracy_score(target.values, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_curve(clf, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "\n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.round(np.linspace(100, len(X_train), 50))\n",
    "    train_f1 = np.zeros(len(sizes))\n",
    "    test_f1 = np.zeros(len(sizes))\n",
    "\n",
    "    for i, s in enumerate(sizes):\n",
    "        # Create and fit the decision tree regressor model\n",
    "        clf.fit(X_train[:int(s)], y_train[:int(s)])\n",
    "\n",
    "        # Find the performance on the training and testing set\n",
    "        train_f1[i] = predict_labels(clf,X_train,y_train,False)\n",
    "        test_f1[i] = predict_labels(clf,X_test,y_test,False)\n",
    "\n",
    "\n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(sizes, train_f1, test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_curve_graph(sizes, train_f1, test_f1):\n",
    "    \"\"\"Plot training and test error as a function of the training size.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Performance vs Training Size')\n",
    "    pl.plot(sizes, test_f1, lw=2, label = 'Test F1 Score')\n",
    "    pl.plot(sizes, train_f1, lw=2, label = 'Train F1 Score')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Training Size')\n",
    "    pl.ylabel('F1 Score')\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_curve(clf, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "    \n",
    "    parameter = [50,100,150,200,250,300]\n",
    "    train_f1 = np.zeros(len(parameter))\n",
    "    test_f1 = np.zeros(len(parameter))\n",
    "    \n",
    "    for i, d in enumerate(parameter):\n",
    "        # Create and fit the decision tree regressor model\n",
    "        clf.min_samples_split = d\n",
    "        clf.fit(X_train, y_train)\n",
    "        # Find the performance on the training and testing set\n",
    "        train_f1[i] = predict_labels(clf,X_train,y_train,False)\n",
    "        test_f1[i] = predict_labels(clf,X_test,y_test,False)\n",
    "\n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(parameter, train_f1, test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_curve_graph(n_estimators, train_f1, test_f1):\n",
    "    \"\"\"Plot training and test error as a function of the depth of the decision tree learn.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Max Depth')\n",
    "    pl.plot(max_depth, test_f1, lw=2, label = 'test error')\n",
    "    pl.plot(max_depth, train_f1, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Max Depth')\n",
    "    pl.ylabel('Error')\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_importances(estimator):\n",
    "    \"\"\"Retrieve or aggregate feature importances from estimator\"\"\"\n",
    "    if hasattr(estimator, \"feature_importances_\"):\n",
    "        importances = estimator.feature_importances_\n",
    "\n",
    "    elif hasattr(estimator, \"coef_\"):\n",
    "        if estimator.coef_.ndim == 1:\n",
    "            importances = np.abs(estimator.coef_)\n",
    "\n",
    "        else:\n",
    "            importances = np.sum(np.abs(estimator.coef_), axis=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"The underlying estimator %s has no `coef_` or \"\n",
    "            \"`feature_importances_` attribute. Either pass a fitted estimator\"\n",
    "            \" to SelectFromModel or call fit before calling transform.\"\n",
    "            % estimator.__class__.__name__)\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ready csv with game data\n",
    "game_data = pd.read_csv('gameData_Diffs_10Minutes.csv',keep_default_na=False)\n",
    "print game_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#game_data = game_data[abs(game_data['Gold'])<=1000]\n",
    "\n",
    "# Data Set for learning\n",
    "X_all = game_data[game_data.columns[-14:]]\n",
    "y_all = game_data['winner']\n",
    "\n",
    "X_all = pd.DataFrame(preprocessing.scale(X_all), index = X_all.index, columns = X_all.columns)\n",
    "\n",
    "# Separate data into train/test samples\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all,y_all,test_size=0.25)\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Overall: '\n",
    "naive_accuracy(X_all,y_all)\n",
    "print 'Train set: '\n",
    "naive_accuracy(X_train,y_train)\n",
    "print 'Test set: '\n",
    "naive_accuracy(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "#clf = LogisticRegression(solver='sag')\n",
    "#clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf = AdaBoostClassifier()\n",
    "#clf = GradientBoostingClassifier()\n",
    "#clf = SVC()\n",
    "train_classifier(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid Search implementation to check the best configuration for Ada Boost \n",
    "tuned_parameters = [{'n_estimators': [1,10,50,100], 'learning_rate': [0.1, 0.3, 0.5, 0.7, 1]}]\n",
    "clf = GridSearchCV(AdaBoostClassifier(),param_grid=tuned_parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate Classifier's performance on Train Data\n",
    "print \"Accuracy for training set: {}\".format(predict_labels(clf, X_train, y_train, True))\n",
    "\n",
    "# Evaluate Classifier's performance on Test Data\n",
    "print \"Accuracy for test set: {}%\".format(predict_labels(clf, X_test, y_test, True))\n",
    "\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate Classifier's performance on Train Data\n",
    "print \"Accuracy for training set: {}\".format(predict_labels(clf, X_train, y_train, True))\n",
    "\n",
    "# Evaluate Classifier's performance on Test Data\n",
    "print \"Accuracy for test set: {}%\".format(predict_labels(clf, X_test, y_test, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 0\n",
    "importance = get_feature_importances(clf)\n",
    "print 'Feature importance'\n",
    "for col in X_all.columns:\n",
    "    print col + ' = ' + str(\"{0:.2f}\".format(importance[x]*100))\n",
    "    x = x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultados = pd.DataFrame()\n",
    "resultados['winner'] = clf.predict(X_all)\n",
    "resultados.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
